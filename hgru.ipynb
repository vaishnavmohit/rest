{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for HGRU implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizontal gated recurrent units (hGRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inrtoduction**\n",
    "* Recurrent neural model ofcontextual interactions developed by MÃ©ly \n",
    "* Model units are indexed by their 2D positions(x,y) and feature channel k. \n",
    "* Neural activity is governed by following differential equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image\\1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X \\in R^{WxHxK}$  feedforward drive (neural response to stimulus)\n",
    "$H^{1} \\in R^{WxHxK}$ recurrent circuit input\n",
    "$H^{2} \\in R^{WxHxK}$ recurrent circuit \n",
    "\n",
    "*Advantages of Inhibition*\n",
    "1. Separately input and output allows shunting (inhibition)\n",
    "2. Excitation --> Acts linearly on the input\n",
    "3. Inhibition --> Brings non linearity on Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W^{I}, W^{E} \\in R^{WxHxK}$ inhibitory and exhibitory hypercolumn connecivity \n",
    "\n",
    "$\\mu ~ \\& ~ \\alpha$ control linear and quadratic inhibition by $C^{1} \\in R^{WxHxK}$\n",
    "\n",
    "$\\gamma$ scales excitation by $C^{2} \\in R^{WxHxK}$\n",
    "\n",
    "$\\xi$ scales the feedforward drive\n",
    "\n",
    "Each stage is linearly rectified (RELU) $[.]_{+} = max(.,0)$\n",
    "\n",
    "$\\eta, \\epsilon, \\tau, \\sigma$ are time constant\n",
    "\n",
    "* set $\\eta = \\tau, \\sigma = \\epsilon\\$\n",
    "\n",
    "$\\triangle t = \\frac{\\eta}{\\epsilon^{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving HGRU from contextual neural circuit model\n",
    "\n",
    "Rearraging the eq (1):\n",
    "<img src=\"image\\4.png\">\n",
    "\n",
    "Discretizing equation using the Eular's method, setting $\\eta = \\tau$ and $\\sigma = \\epsilon$\n",
    "\n",
    "<img src=\"image\\5.png\">\n",
    "\n",
    "Aplying with time steps *h*\n",
    "\n",
    "<img src=\"image\\6.png\">\n",
    "\n",
    "\n",
    "\n",
    "Distributing *h*\n",
    "\n",
    "<img src=\"image\\7.png\">\n",
    "\n",
    "\n",
    "\n",
    "choosing $h = \\frac{\\eta}{\\epsilon^2}$\n",
    "\n",
    "<img src=\"image\\8.png\">\n",
    "\n",
    "Discrete time approximation of initial dynamical system:\n",
    "* Because RNN are difficult to train , gates manage the flow of information over time. \n",
    "\n",
    "<img src=\"image\\9.png\">\n",
    "\n",
    "$\\sigma$ is the squashing pointwise nonlinearity\n",
    "\n",
    "$U^{(.)}$ -> Convolutional kernal\n",
    "\n",
    "$b^{(.)}$ -> Bias\n",
    "\n",
    "When this gate is applied to the above equation we obtain:\n",
    "\n",
    "\n",
    "<img src=\"image\\10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Initializing the parameters:\n",
    "\n",
    "self.u1_gate = nn.Conv2d(hidden_size, hidden_size, 1)\n",
    "self.u2_gate = nn.Conv2d(hidden_size, hidden_size, 1)\n",
    "        \n",
    "self.w_gate_inh = nn.Parameter(torch.empty(hidden_size , hidden_size , kernel_size, kernel_size))\n",
    "self.w_gate_exc = nn.Parameter(torch.empty(hidden_size , hidden_size , kernel_size, kernel_size))\n",
    "\n",
    "self.alpha = nn.Parameter(torch.empty((hidden_size,1,1)))\n",
    "self.gamma = nn.Parameter(torch.empty((hidden_size,1,1)))\n",
    "self.kappa = nn.Parameter(torch.empty((hidden_size,1,1)))\n",
    "self.w = nn.Parameter(torch.empty((hidden_size,1,1)))\n",
    "self.mu= nn.Parameter(torch.empty((hidden_size,1,1)))\n",
    "\n",
    "# Defining the parameters:\n",
    "\n",
    "i = timestep # i = 8\n",
    "if self.batchnorm:\n",
    "    g1_t = torch.sigmoid(self.bn[i*4+0](self.u1_gate(prev_state2)))\n",
    "    c1_t = self.bn[i*4+1](F.conv2d(prev_state2 * g1_t, self.w_gate_inh, padding=self.padding))\n",
    "\n",
    "    next_state1 = F.relu(input_ - F.relu(c1_t*(self.alpha*prev_state2 + self.mu)))\n",
    "\n",
    "    g2_t = torch.sigmoid(self.bn[i*4+2](self.u2_gate(next_state1)))\n",
    "    c2_t = self.bn[i*4+3](F.conv2d(next_state1, self.w_gate_exc, padding=self.padding))\n",
    "    h2_t = F.relu(self.kappa*next_state1 + self.gamma*c2_t + self.w*next_state1*c2_t)\n",
    "\n",
    "    prev_state2 = (1 - g2_t)*prev_state2 + g2_t*h2_t\n",
    "\n",
    "else:\n",
    "    g1_t = F.sigmoid(self.u1_gate(prev_state2))\n",
    "    c1_t = F.conv2d(prev_state2 * g1_t, self.w_gate_inh, padding=self.padding)\n",
    "    \n",
    "    next_state1 = F.tanh(input_ - c1_t*(self.alpha*prev_state2 + self.mu))\n",
    "    \n",
    "    g2_t = F.sigmoid(self.bn[i*4+2](self.u2_gate(next_state1)))\n",
    "    c2_t = F.conv2d(next_state1, self.w_gate_exc, padding=self.padding)\n",
    "    h2_t = F.tanh(self.kappa*(next_state1 + self.gamma*c2_t) + (self.w*(next_state1*(self.gamma*c2_t))))\n",
    "    \n",
    "    prev_state2 = self.n[timestep]*((1 - g2_t)*prev_state2 + g2_t*h2_t)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainable convolutional recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image\\2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HGRU formulation\n",
    "\n",
    "Model build on equation 2 has the capacity to learn complex interaction between units via horizontal connection \n",
    "\n",
    "Modification intrduced in equation 2\n",
    "* learnable gates (GRU like)\n",
    "* $H^2$ (excitation) is symmetric with $H^1$ (inhibition) -> gives ability to learn in implementing linear and interaction at each processing stages\n",
    "* To control unstable gradient -> squashing pointwise non linearity and learned parameter to globally scale activity at every processing time step.\n",
    "\n",
    "<img src=\"image\\3.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* HGRU can learn non linear interaction between spatially neighbouring units in feedforward drive **X** enoded in hidden state *$H^2$*\n",
    "\n",
    "Stage 1:\n",
    "1. Horizontal inhibition (blue) calculated by applying gain to $H^2[t-1]$ and convolving with W\n",
    "2. Linear ($+$) and quadratic ($\\times$) control the convergence of inhibition onto $X$\n",
    "\n",
    "Stage 2:\n",
    "1. Horizontal excitation is computed by convolving $H^1[t]$ with W\n",
    "2. Another linear and quadratic controller\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward drive **X** corresponds to activity from preceesing convolutional layer\n",
    "\n",
    "**HGRU** encodes the spatial dependency via time varying hidden states ($H^1$ and $H^2$)\n",
    "\n",
    "These states are updated via reset and update gates ($G^1$ and $G^2$)\n",
    "\n",
    "Gates are derived from convolutions between Kernals ($U^1 and U^2 \\in R^{1x1xKxK}$ ) and hidden states shifted by bias. \n",
    "\n",
    "Point wise non linearity ($\\sigma$) for normalization\n",
    "\n",
    "<img src=\"image\\12.png\">\n",
    "\n",
    "\n",
    "Horizontal interaction between units are calculated by kernel **W** $\\in R^{SxSxKxK}$, where S is the spatial extent of these connections in single time step. \n",
    "\n",
    "**W** is contrained to have symmetric weights between the channel -> reduces the numer of learnable parameters by half. \n",
    "\n",
    "Hidden states are recomputed via horizontal interaction at each time step $t \\in [0, T]$\n",
    "\n",
    "$H^2[t-1]$ is modulated by gain $G^1[t]$ which gives $C^1[t]$ after conlcolving with W\n",
    "\n",
    "$C^1[t]$ is horizontal inhibition of hGRU which is applied to **X** via $\\mu and \\sigma$\n",
    "\n",
    "They are *k* dimensional -> scaling linear and quadratic terms of horizontal interaction with **X**\n",
    "\n",
    "Pointwise $\\xi$ is hyperbolic tangent for normalization in range [-1,1]\n",
    "\n",
    "<img src=\"image\\13.png\">\n",
    "\n",
    "$C^2[t]$ represents excitation of horizontal connection\n",
    "\n",
    "Linear and quadratic contributions are managed by $\\kappa, \\omega, \\beta$\n",
    "\n",
    "Learnable T dimensional $\\eta$ time gain, helps to control unstable gradient during training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing Dataset:\n",
    "\n",
    "### Path finder challenge\n",
    "\n",
    "* Synthetic visual task inspired by visual psychology\n",
    "\n",
    "* Task is to detect if two circles are connected by a path or not (**Binary Classification**)\n",
    "\n",
    "* Multiple shorter paths present -> **Distractor Path**\n",
    "\n",
    "* AIM -> detect the long range of spatial dependencies \n",
    "\n",
    "**Positive examples** placing two circles at the end of single path and **Negative** on different path\n",
    "\n",
    "Three types: Peddler length **6,9 and 14**\n",
    "\n",
    "<img src=\"image\\14.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter selected:\n",
    "\n",
    "**Recurrent Model**\n",
    "* 8 time steps\n",
    "* 15x15 horizontal connection Kernels (**W**)\n",
    "\n",
    "*Observation*\n",
    "\n",
    "* path length increased -> performance decreased\n",
    "* more time steps are required wrt increasing length of the path. -> Human wrt distance.\n",
    "\n",
    "**Feedforward Model**\n",
    "* Three types of kernels used: 10x10, 15x15, 20x20 (small, medium and large)\n",
    "* performance is compared keeping number of parameters same as that of hGRU (36, 16 and 9 kernels)\n",
    "* Other combination-> 2 pixel dialated convolution before convolving input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including all the header files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torchvision.transforms import Compose as transcompose\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "\n",
    "from dataset import DataSetPol\n",
    "from hgru import hConvGRU, FFConvNet\n",
    "from transforms import GroupScale, Augmentation, Stack, ToTorchFormatTensor\n",
    "from misc_functions import AverageMeter, accuracy, plot_grad_flow\n",
    "from statistics import mean\n",
    "\n",
    "#from opts import parser\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = 'train_images_9.txt'\n",
    "val_list = 'test_images_9.txt'\n",
    "name = 'hgru'\n",
    "\n",
    "# ========================= Learning Configs ==========================\n",
    "epochs = 10\n",
    "batch_size = 64 #batch-size\n",
    "lr = .001 #learning rate\n",
    "lr_steps = [20, 40] #epochs to decay learning rate by 10\n",
    "\n",
    "# ========================= Monitor Configs ==========================\n",
    "print_freq = 200 #print-frequency\n",
    "ef = 1 #eval frequency\n",
    "parallel = True\n",
    "start_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset\n",
      "Loading validation dataset\n",
      "device used is:  cuda\n"
     ]
    }
   ],
   "source": [
    "plt.ion()\n",
    "plt.show()\n",
    "\n",
    "global best_prec1\n",
    "best_prec1 = 0\n",
    "\n",
    "transform_list = transcompose([GroupScale((150,150)), Augmentation(), Stack(), \n",
    "                               ToTorchFormatTensor(div=True)])\n",
    "\n",
    "print(\"Loading training dataset\")\n",
    "train_loader = torch.utils.data.DataLoader(DataSetPol(\"/media/data_cifs/curvy_2snakes_300/\", \n",
    "                                                      train_list, \n",
    "                                                      transform = transform_list ),\n",
    "                                           batch_size=batch_size,   \n",
    "                                           shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(\"Loading validation dataset\")\n",
    "val_loader = torch.utils.data.DataLoader(DataSetPol(\"/media/data_cifs/curvy_2snakes_300/\", \n",
    "                                                    val_list, transform = transform_list),\n",
    "                                         batch_size=batch_size, shuffle=False, \n",
    "                                         num_workers=4, pin_memory=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device used is: ',device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, iter, criterion, logger=None):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            imgs = imgs.cuda()\n",
    "            output = model.forward(imgs)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            losses.update(loss.data, imgs.size(0))\n",
    "            \n",
    "            [prec1] = accuracy(output.data, target, topk=(1,))\n",
    "            top1.update(prec1, imgs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t Time: {batch_time.avg:.3f}\\t Loss: {loss.val:.4f} ({loss.avg: .4f})\\t'\n",
    "                       'Prec: {top1.val:.3f} ({top1.avg:.3f})\\t'.format(i, len(val_loader), \n",
    "                                                                        batch_time=batch_time, \n",
    "                                                                        loss=losses, top1=top1))\n",
    "            \n",
    "    print('Testing Results: Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Loss {loss.avg:.5f}'\n",
    "          .format(top1=top1, top5=top5, loss=losses))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    filename = '_'.join((name, 'accuracy', str(state['best_prec1'].item()), \n",
    "                         'epoch', str(state['epoch']), filename))\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model\n",
      "Training with filter size: 15 x 15\n",
      "Loading parallel finished on GPU count: 8\n",
      "Starting training: \n",
      "Epoch: [0][0/7422]\t lr: 0.001\t Time: 3.838 (3.838)\t Data: 2.766 (2.766)\tPrec: 56.250 (56.250) (56.250)\t Loss: 0.711636 (0.711636) (0.711636)\n",
      "Epoch: [0][200/7422]\t lr: 0.001\t Time: 1.482 (1.356)\t Data: 0.000 (0.014)\tPrec: 57.812 (51.516) (51.539)\t Loss: 0.683204 (0.701643) (0.701692)\n",
      "Epoch: [0][400/7422]\t lr: 0.001\t Time: 1.503 (1.409)\t Data: 0.000 (0.007)\tPrec: 50.000 (51.188) (51.364)\t Loss: 0.688915 (0.694152) (0.697932)\n",
      "Epoch: [0][600/7422]\t lr: 0.001\t Time: 1.494 (1.429)\t Data: 0.000 (0.005)\tPrec: 57.812 (52.953) (51.893)\t Loss: 0.685336 (0.691379) (0.695751)\n",
      "Epoch: [0][800/7422]\t lr: 0.001\t Time: 1.510 (1.438)\t Data: 0.000 (0.004)\tPrec: 37.500 (53.195) (52.218)\t Loss: 0.709131 (0.691124) (0.694596)\n",
      "Epoch: [0][1000/7422]\t lr: 0.001\t Time: 1.400 (1.443)\t Data: 0.000 (0.003)\tPrec: 51.562 (53.992) (52.572)\t Loss: 0.689172 (0.690171) (0.693712)\n",
      "Epoch: [0][1200/7422]\t lr: 0.001\t Time: 1.480 (1.446)\t Data: 0.000 (0.003)\tPrec: 54.688 (53.656) (52.753)\t Loss: 0.687224 (0.690128) (0.693115)\n",
      "Epoch: [0][1400/7422]\t lr: 0.001\t Time: 1.515 (1.450)\t Data: 0.000 (0.002)\tPrec: 51.562 (53.406) (52.846)\t Loss: 0.687280 (0.689867) (0.692651)\n",
      "Epoch: [0][1600/7422]\t lr: 0.001\t Time: 1.489 (1.453)\t Data: 0.000 (0.002)\tPrec: 50.000 (53.727) (52.956)\t Loss: 0.705824 (0.690158) (0.692340)\n",
      "Epoch: [0][1800/7422]\t lr: 0.001\t Time: 1.494 (1.455)\t Data: 0.000 (0.002)\tPrec: 48.438 (53.812) (53.051)\t Loss: 0.701783 (0.689380) (0.692011)\n",
      "Epoch: [0][2000/7422]\t lr: 0.001\t Time: 1.504 (1.457)\t Data: 0.000 (0.002)\tPrec: 48.438 (53.125) (53.059)\t Loss: 0.693307 (0.689803) (0.691790)\n",
      "Epoch: [0][2200/7422]\t lr: 0.001\t Time: 1.498 (1.459)\t Data: 0.000 (0.001)\tPrec: 54.688 (53.344) (53.085)\t Loss: 0.682085 (0.688907) (0.691528)\n",
      "Epoch: [0][2400/7422]\t lr: 0.001\t Time: 1.503 (1.460)\t Data: 0.000 (0.001)\tPrec: 50.000 (54.078) (53.167)\t Loss: 0.700457 (0.688054) (0.691239)\n",
      "Epoch: [0][2600/7422]\t lr: 0.001\t Time: 1.488 (1.461)\t Data: 0.000 (0.001)\tPrec: 48.438 (54.188) (53.246)\t Loss: 0.701780 (0.686750) (0.690894)\n",
      "Epoch: [0][2800/7422]\t lr: 0.001\t Time: 1.495 (1.462)\t Data: 0.000 (0.001)\tPrec: 64.062 (53.609) (53.272)\t Loss: 0.669128 (0.688470) (0.690721)\n",
      "Epoch: [0][3000/7422]\t lr: 0.001\t Time: 1.525 (1.463)\t Data: 0.000 (0.001)\tPrec: 53.125 (53.180) (53.266)\t Loss: 0.686876 (0.689047) (0.690609)\n",
      "Epoch: [0][3200/7422]\t lr: 0.001\t Time: 1.482 (1.464)\t Data: 0.000 (0.001)\tPrec: 68.750 (54.469) (53.341)\t Loss: 0.662802 (0.687046) (0.690387)\n",
      "Epoch: [0][3400/7422]\t lr: 0.001\t Time: 1.499 (1.465)\t Data: 0.000 (0.001)\tPrec: 59.375 (54.195) (53.391)\t Loss: 0.660916 (0.687226) (0.690201)\n",
      "Epoch: [0][3600/7422]\t lr: 0.001\t Time: 1.478 (1.465)\t Data: 0.000 (0.001)\tPrec: 56.250 (54.164) (53.434)\t Loss: 0.682916 (0.686563) (0.689999)\n",
      "Epoch: [0][3800/7422]\t lr: 0.001\t Time: 1.433 (1.466)\t Data: 0.000 (0.001)\tPrec: 56.250 (53.648) (53.445)\t Loss: 0.679565 (0.688467) (0.689918)\n",
      "Epoch: [0][4000/7422]\t lr: 0.001\t Time: 1.487 (1.467)\t Data: 0.000 (0.001)\tPrec: 51.562 (53.406) (53.443)\t Loss: 0.670218 (0.687041) (0.689774)\n",
      "Epoch: [0][4200/7422]\t lr: 0.001\t Time: 1.499 (1.467)\t Data: 0.000 (0.001)\tPrec: 51.562 (54.523) (53.495)\t Loss: 0.697146 (0.683237) (0.689463)\n",
      "Epoch: [0][4400/7422]\t lr: 0.001\t Time: 1.525 (1.467)\t Data: 0.000 (0.001)\tPrec: 53.125 (54.133) (53.524)\t Loss: 0.680384 (0.685612) (0.689288)\n",
      "Epoch: [0][4600/7422]\t lr: 0.001\t Time: 1.501 (1.468)\t Data: 0.000 (0.001)\tPrec: 62.500 (54.656) (53.573)\t Loss: 0.676714 (0.684092) (0.689062)\n",
      "Epoch: [0][4800/7422]\t lr: 0.001\t Time: 1.356 (1.468)\t Data: 0.000 (0.001)\tPrec: 56.250 (54.812) (53.625)\t Loss: 0.709651 (0.684492) (0.688872)\n",
      "Epoch: [0][5000/7422]\t lr: 0.001\t Time: 1.514 (1.469)\t Data: 0.000 (0.001)\tPrec: 65.625 (55.070) (53.682)\t Loss: 0.656134 (0.681932) (0.688594)\n",
      "Epoch: [0][5200/7422]\t lr: 0.001\t Time: 1.497 (1.469)\t Data: 0.000 (0.001)\tPrec: 64.062 (55.492) (53.752)\t Loss: 0.666067 (0.681150) (0.688308)\n",
      "Epoch: [0][5400/7422]\t lr: 0.001\t Time: 1.506 (1.470)\t Data: 0.000 (0.001)\tPrec: 53.125 (55.305) (53.809)\t Loss: 0.698019 (0.680891) (0.688033)\n",
      "Epoch: [0][5600/7422]\t lr: 0.001\t Time: 1.510 (1.470)\t Data: 0.000 (0.001)\tPrec: 67.188 (55.766) (53.879)\t Loss: 0.637019 (0.677468) (0.687656)\n",
      "Epoch: [0][5800/7422]\t lr: 0.001\t Time: 1.488 (1.470)\t Data: 0.000 (0.001)\tPrec: 59.375 (55.922) (53.950)\t Loss: 0.657886 (0.677245) (0.687297)\n",
      "Epoch: [0][6000/7422]\t lr: 0.001\t Time: 1.485 (1.470)\t Data: 0.000 (0.001)\tPrec: 57.812 (55.984) (54.018)\t Loss: 0.684855 (0.670764) (0.686746)\n",
      "Epoch: [0][6200/7422]\t lr: 0.001\t Time: 1.500 (1.470)\t Data: 0.000 (0.001)\tPrec: 56.250 (56.719) (54.105)\t Loss: 0.662106 (0.665920) (0.686074)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #global best_prec1\n",
    "\n",
    "    print(\"Init model\")\n",
    "    if parallel == True:\n",
    "        model = hConvGRU(timesteps=8, filt_size = 15)\n",
    "        model = torch.nn.DataParallel(model).to(device)\n",
    "        print(\"Loading parallel finished on GPU count:\", torch.cuda.device_count())\n",
    "    else:\n",
    "        model = hConvGRU(timesteps=8, filt_size = 15).to(device)\n",
    "        print(\"Loading finished\")\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    lr_init = lr\n",
    "    print(\"Starting training: \")\n",
    "    f_val= []\n",
    "    f_training = []\n",
    "    train_loss_history = []\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        \n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "    \n",
    "        model.train()\n",
    "        end = time.perf_counter() #like a stop watch for the code\n",
    "        for i, (imgs, target) in enumerate(train_loader):\n",
    "            data_time.update(time.perf_counter() - end)\n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output  = model.forward(imgs) \n",
    "            '''\n",
    "            This passes the images through the model (forward pass) and applies the operations previously \n",
    "            discussed in layer. You get the resultant output.\n",
    "            '''\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            [prec1] = accuracy(output.data, target, topk=(1,))\n",
    "            \n",
    "            losses.update(loss.data.item(), imgs.size(0))\n",
    "            top1.update(prec1.data.item(), imgs.size(0))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_time.update(time.perf_counter() - end)\n",
    "            \n",
    "            end = time.perf_counter()\n",
    "            if i % (print_freq) == 0:\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t lr: {lr:g}\\t Time: {batch_time.val:.3f} ({batch_time.avg:.3f})\\t Data: {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                       'Prec: {top1.val:.3f} ({precprint:.3f}) ({top1.avg:.3f})\\t Loss: {loss.val:.6f} ({lossprint:.6f}) ({loss.avg:.6f})'.format(epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                        data_time=data_time, loss=losses, lossprint= mean(losses.history[-print_freq:]), lr=lr, top1=top1, precprint= mean(top1.history[-print_freq:])))\n",
    "            \n",
    "        f_training.append(top1.avg)\n",
    "        train_loss_history += losses.history\n",
    "        if (epoch + 1) % 1 == 0 or epoch == epochs - 1:\n",
    "            prec = validate(val_loader, model, (epoch + 1) * len(train_loader), criterion)\n",
    "            f_val.append(prec)\n",
    "            is_best = prec > best_prec1\n",
    "            if is_best:\n",
    "                best_prec1 = max(prec, best_prec1)\n",
    "                save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_prec1': best_prec1,\n",
    "                }, is_best)\n",
    "\n",
    "    np.array(f_training).dump(open(\"{}.npy\".format(name),'w'))\n",
    "    np.array(f_val).dump(open(\"{}.npy\".format(name),'w'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('gabor_serre.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 1, 7, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hConvGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, timesteps=8, filt_size = 9):\n",
    "        super().__init__()\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(1, 25, kernel_size=7, padding=3)\n",
    "        part1 = np.load(\"gabor_serre.npy\")\n",
    "        self.conv0.weight.data = torch.FloatTensor(part1)\n",
    "        \n",
    "        # calling hconvgrucell\n",
    "        \n",
    "        self.unit1 = hConvGRUCell(25, 25, filt_size) \n",
    "        print(\"Training with filter size:\",filt_size,\"x\",filt_size)\n",
    "        self.unit1.train()\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(25, eps=1e-03)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(25, 2, kernel_size=1)\n",
    "        init.xavier_normal_(self.conv6.weight)\n",
    "        init.constant_(self.conv6.bias, 0)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(150, stride=1)\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(2, eps=1e-03)\n",
    "        \n",
    "        self.fc = nn.Linear(2, 2)\n",
    "        init.xavier_normal_(self.fc.weight)\n",
    "        init.constant_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        internal_state = None\n",
    "\n",
    "        x = self.conv0(x)\n",
    "        x = torch.pow(x, 2)\n",
    "        \n",
    "        for i in range(self.timesteps):\n",
    "            internal_state  = self.unit1(x, internal_state, timestep=i)\n",
    "        output = self.bn(internal_state)\n",
    "        output = F.leaky_relu(self.conv6(output))\n",
    "        output = self.maxpool(output)\n",
    "        output = self.bn2(output)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hConvGRUCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Generate a convolutional GRU cell\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, kernel_size, batchnorm=True, timesteps=8):\n",
    "        super().__init__()\n",
    "        self.padding = kernel_size // 2\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.timesteps = timesteps\n",
    "        self.batchnorm = batchnorm\n",
    "        \n",
    "        self.u1_gate = nn.Conv2d(hidden_size, hidden_size, 1)\n",
    "        self.u2_gate = nn.Conv2d(hidden_size, hidden_size, 1)\n",
    "        \n",
    "        self.w_gate_inh = nn.Parameter(torch.empty(hidden_size , hidden_size , kernel_size, kernel_size))\n",
    "        self.w_gate_exc = nn.Parameter(torch.empty(hidden_size , hidden_size , kernel_size, kernel_size))\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.empty((hidden_size,1,1)))\n",
    "        self.gamma = nn.Parameter(torch.empty((hidden_size,1,1)))\n",
    "        self.kappa = nn.Parameter(torch.empty((hidden_size,1,1)))\n",
    "        self.w = nn.Parameter(torch.empty((hidden_size,1,1)))\n",
    "        self.mu= nn.Parameter(torch.empty((hidden_size,1,1)))\n",
    "\n",
    "        if self.batchnorm:\n",
    "            self.bn = nn.ModuleList([nn.BatchNorm2d(25, eps=1e-03) for i in range(32)])\n",
    "        else:\n",
    "            self.n = nn.Parameter(torch.randn(self.timesteps,1,1))\n",
    "\n",
    "        init.orthogonal_(self.w_gate_inh)\n",
    "        init.orthogonal_(self.w_gate_exc)\n",
    "        \n",
    "        self.w_gate_inh.register_hook(lambda grad: (grad + torch.transpose(grad,1,0))*0.5)\n",
    "        self.w_gate_exc.register_hook(lambda grad: (grad + torch.transpose(grad,1,0))*0.5)\n",
    "\n",
    "        \n",
    "        init.orthogonal_(self.u1_gate.weight)\n",
    "        init.orthogonal_(self.u2_gate.weight)\n",
    "        \n",
    "        for bn in self.bn:\n",
    "            init.constant_(bn.weight, 0.1)\n",
    "        \n",
    "        init.constant_(self.alpha, 0.1)\n",
    "        init.constant_(self.gamma, 1.0)\n",
    "        init.constant_(self.kappa, 0.5)\n",
    "        init.constant_(self.w, 0.5)\n",
    "        init.constant_(self.mu, 1)\n",
    "        \n",
    "        init.uniform_(self.u1_gate.bias.data, 1, 8.0 - 1)\n",
    "        self.u1_gate.bias.data.log()\n",
    "        self.u2_gate.bias.data =  -self.u1_gate.bias.data\n",
    "\n",
    "\n",
    "    def forward(self, input_, prev_state2, timestep=0):\n",
    "\n",
    "        if timestep == 0:\n",
    "            prev_state2 = torch.empty_like(input_)\n",
    "            init.xavier_normal_(prev_state2)\n",
    "\n",
    "        #import pdb; pdb.set_trace()\n",
    "        i = timestep\n",
    "        if self.batchnorm:\n",
    "            g1_t = torch.sigmoid(self.bn[i*4+0](self.u1_gate(prev_state2)))\n",
    "            c1_t = self.bn[i*4+1](F.conv2d(prev_state2 * g1_t, self.w_gate_inh, padding=self.padding))\n",
    "            \n",
    "            next_state1 = F.relu(input_ - F.relu(c1_t*(self.alpha*prev_state2 + self.mu)))\n",
    "            \n",
    "            g2_t = torch.sigmoid(self.bn[i*4+2](self.u2_gate(next_state1)))\n",
    "            c2_t = self.bn[i*4+3](F.conv2d(next_state1, self.w_gate_exc, padding=self.padding))\n",
    "            \n",
    "            h2_t = F.relu(self.kappa*next_state1 + self.gamma*c2_t + self.w*next_state1*c2_t)\n",
    "            \n",
    "            prev_state2 = (1 - g2_t)*prev_state2 + g2_t*h2_t\n",
    "\n",
    "        else:\n",
    "            g1_t = F.sigmoid(self.u1_gate(prev_state2))\n",
    "            c1_t = F.conv2d(prev_state2 * g1_t, self.w_gate_inh, padding=self.padding)\n",
    "            next_state1 = F.tanh(input_ - c1_t*(self.alpha*prev_state2 + self.mu))\n",
    "            g2_t = F.sigmoid(self.bn[i*4+2](self.u2_gate(next_state1)))\n",
    "            c2_t = F.conv2d(next_state1, self.w_gate_exc, padding=self.padding)\n",
    "            h2_t = F.tanh(self.kappa*(next_state1 + self.gamma*c2_t) + (self.w*(next_state1*(self.gamma*c2_t))))\n",
    "            prev_state2 = self.n[timestep]*((1 - g2_t)*prev_state2 + g2_t*h2_t)\n",
    "\n",
    "        return prev_state2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with filter size: 15 x 15\n",
      "hConvGRU(\n",
      "  (conv0): Conv2d(1, 25, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "  (unit1): hConvGRUCell(\n",
      "    (u1_gate): Conv2d(25, 25, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (u2_gate): Conv2d(25, 25, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (bn): ModuleList(\n",
      "      (0): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (7): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (9): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (11): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (12): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (13): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (14): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (15): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (16): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (17): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (18): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (19): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (20): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (21): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (22): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (23): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (24): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (25): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (26): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (27): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (28): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (29): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (30): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (31): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (bn): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv6): Conv2d(25, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (maxpool): MaxPool2d(kernel_size=150, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn2): BatchNorm2d(2, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = hConvGRU(timesteps=8, filt_size = 15)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hConvGRU(\n",
       "  (conv0): Conv2d(1, 25, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  (unit1): hConvGRUCell(\n",
       "    (u1_gate): Conv2d(25, 25, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (u2_gate): Conv2d(25, 25, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn): ModuleList(\n",
       "      (0): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (13): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (15): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (16): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (17): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (18): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (19): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (20): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (21): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (22): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (23): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (24): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (25): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (26): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (27): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (28): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (29): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (30): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (31): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (bn): BatchNorm2d(25, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(25, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (maxpool): MaxPool2d(kernel_size=150, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bn2): BatchNorm2d(2, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=2, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vreason",
   "language": "python",
   "name": "vreason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
